# Multi-Step Prompting Workflow for Test Generation
# Based on the Explication-Planning-Refinement-Implementation pattern

workflows:
  generate_unit_tests:
    steps:
      - name: explication
        prompt: |
          ### {{function_name}} ###
          Can you explain what this function does?

          Code:
          ```{{language}}
          {{code}}
          ```

          Please analyze:
          1. The function's purpose and responsibility
          2. Input parameters and their types
          3. Return value and its structure
          4. Side effects (if any)
          5. Dependencies and external calls

      - name: planning
        prompt: |
          Based on the analysis above, can you plan a set of unit tests for {{function_name}}?

          Consider:
          1. Happy path scenarios
          2. Edge cases (boundary values, null/undefined, empty collections)
          3. Error conditions
          4. Integration points
          5. Performance characteristics

          Output a test plan with:
          - Test case name (following {{naming_convention}})
          - Description
          - Input data
          - Expected output
          - Assertions to verify

      - name: refinement
        prompt: |
          Review the test plan above for completeness:
          1. Are there any missing edge cases?
          2. Are all error paths covered?
          3. Is the test coverage sufficient?
          4. Are the tests independent and isolated?
          5. Can any tests be combined or split?

          Suggest improvements to the test plan.

      - name: implementation
        prompt: |
          Generate the implementation of the tests using {{test_framework}}.

          Requirements:
          - Follow the naming convention: {{naming_convention}}
          - Use {{assertion_library}} for assertions
          - Mock all external dependencies
          - Include setup and teardown where needed
          - Add descriptive comments for complex test logic
          - Ensure tests are deterministic and repeatable

          Generate each test as a separate, runnable test case.

  generate_integration_tests:
    steps:
      - name: system_analysis
        prompt: |
          Analyze the integration between {{service_a}} and {{service_b}}:

          1. What is the communication protocol?
          2. What are the API contracts?
          3. What data is exchanged?
          4. What are the success scenarios?
          5. What are the failure scenarios?
          6. How are errors handled?

      - name: test_design
        prompt: |
          Design integration tests for the {{service_a}} → {{service_b}} flow.

          Consider:
          1. Contract validation
          2. Data transformation correctness
          3. Error propagation
          4. Timeout handling
          5. Retry logic
          6. Circuit breaker behavior (if applicable)

      - name: scenario_generation
        prompt: |
          Generate concrete test scenarios:

          For each scenario, provide:
          - Scenario name
          - Pre-conditions (database state, service state)
          - Test steps
          - Expected results
          - Cleanup requirements

  generate_security_tests:
    steps:
      - name: threat_modeling
        prompt: |
          Perform threat modeling for {{component}}:

          1. Identify all input vectors (HTTP params, WebSocket messages, etc.)
          2. Identify data storage points
          3. Identify authentication/authorization checks
          4. Identify external API calls
          5. Identify sensitive data exposure points

      - name: attack_simulation
        prompt: |
          Design security tests for the identified threats:

          For each attack vector:
          - OWASP category (e.g., A01:2021 – Broken Access Control)
          - Attack method
          - Expected mitigation
          - Test implementation

          Cover:
          - SQL Injection
          - XSS
          - CSRF
          - Authentication bypass
          - Authorization escalation
          - DoS vulnerabilities
          - Sensitive data exposure

  generate_e2e_tests:
    steps:
      - name: user_journey_mapping
        prompt: |
          Map out the user journey for "{{feature}}":

          1. User entry point
          2. Sequence of actions
          3. Decision points
          4. Success criteria
          5. Alternative paths
          6. Error recovery paths

      - name: test_scenario_design
        prompt: |
          Design end-to-end test scenarios for the user journey.

          For each scenario:
          - User persona and context
          - Starting state (logged in/out, device type, etc.)
          - Step-by-step actions
          - Verification points
          - Expected outcome

      - name: automation_implementation
        prompt: |
          Generate Playwright test code for the E2E scenarios.

          Requirements:
          - Use Playwright API
          - Include page object pattern where applicable
          - Add explicit waits and assertions
          - Handle dynamic content
          - Include accessibility checks
          - Add screenshots on failure
          - Make tests retry-safe

# Test naming conventions
naming_conventions:
  typescript:
    unit: "describe('{{Component}}', () => { it('{{action}} when {{state}}', async () => {}) })"
    integration: "describe('{{Service}} Integration', () => { it('{{scenario}}', async () => {}) })"
    e2e: "test('{{user_story}}', async ({ page }) => {})"

  rust:
    unit: "#[test]\nfn test_{{function}}_{{scenario}}() {}"
    integration: "#[tokio::test]\nasync fn test_{{integration}}_{{scenario}}() {}"

# Assertion libraries and frameworks
frameworks:
  typescript:
    unit: ["vitest", "@testing-library/react", "@testing-library/user-event"]
    integration: ["vitest", "msw", "supertest"]
    e2e: ["@playwright/test"]

  rust:
    unit: ["cargo test", "mockall"]
    integration: ["cargo test", "tokio::test", "wiremock"]
